{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52a67ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境准备完毕\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.transforms as transforms\n",
    "import mindspore.dataset.vision as vision\n",
    "from mindspore import context, Tensor\n",
    "from mindspore.train import Model\n",
    "from mindspore.train.callback import LossMonitor, TimeMonitor, Callback\n",
    "import mindspore.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import urllib.request\n",
    "\n",
    "print(\"环境准备完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3382b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置运行模式和设备\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1889bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url, filename, extract_to):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f'Downloading {filename}...')\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f'{filename} downloaded.')\n",
    "    if not os.path.exists(extract_to):\n",
    "        with gzip.open(filename, 'rb') as f_in:\n",
    "            with open(extract_to, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(f'{filename} extracted to {extract_to}.')\n",
    "\n",
    "def create_fashion_mnist_csv(data_path):\n",
    "    urls = {\n",
    "        'train_images': 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz',\n",
    "        'test_images': 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz',\n",
    "        'test_labels': 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "    paths = {\n",
    "        'train_images': os.path.join(data_path, 'train-images-idx3-ubyte.gz'),\n",
    "        'train_labels': os.path.join(data_path, 'train-labels-idx1-ubyte.gz'),\n",
    "        'test_images': os.path.join(data_path, 't10k-images-idx3-ubyte.gz'),\n",
    "        'test_labels': os.path.join(data_path, 't10k-labels-idx1-ubyte.gz')\n",
    "    }\n",
    "    extract_paths = {\n",
    "        'train_images': os.path.join(data_path, 'train-images-idx3-ubyte'),\n",
    "        'train_labels': os.path.join(data_path, 'train-labels-idx1-ubyte'),\n",
    "        'test_images': os.path.join(data_path, 't10k-images-idx3-ubyte'),\n",
    "        'test_labels': os.path.join(data_path, 't10k-labels-idx1-ubyte')\n",
    "    }\n",
    "    \n",
    "    for key in urls:\n",
    "        download_and_extract(urls[key], paths[key], extract_paths[key])\n",
    "    \n",
    "    def read_images(file):\n",
    "        with open(file, 'rb') as f:\n",
    "            f.read(16)\n",
    "            buf = f.read()\n",
    "            data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "            data = data.reshape(-1, 28 * 28)\n",
    "            return data\n",
    "    \n",
    "    def read_labels(file):\n",
    "        with open(file, 'rb') as f:\n",
    "            f.read(8)\n",
    "            buf = f.read()\n",
    "            labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int32)\n",
    "            return labels\n",
    "    \n",
    "    def save_to_csv(images, labels, csv_file):\n",
    "        with open(csv_file, 'w') as f:\n",
    "            f.write('label,' + ','.join(['pixel' + str(i) for i in range(28 * 28)]) + '\\n')\n",
    "            for i in range(len(images)):\n",
    "                f.write(str(labels[i]) + ',' + ','.join(map(str, images[i])) + '\\n')\n",
    "        print(f\"Saved {len(images)} entries to {csv_file}\")\n",
    "\n",
    "    train_images = read_images(extract_paths['train_images'])\n",
    "    train_labels = read_labels(extract_paths['train_labels'])\n",
    "    test_images = read_images(extract_paths['test_images'])\n",
    "    test_labels = read_labels(extract_paths['test_labels'])\n",
    "    \n",
    "    save_to_csv(train_images, train_labels, os.path.join(data_path, 'fashion-mnist_train.csv'))\n",
    "    save_to_csv(test_images, test_labels, os.path.join(data_path, 'fashion-mnist_test.csv'))\n",
    "\n",
    "def create_csv_dataset(data_path, batch_size=32, repeat_size=1, num_parallel_workers=1, is_train=True):\n",
    "    csv_file = os.path.join(data_path, \"fashion-mnist_train.csv\" if is_train else \"fashion-mnist_test.csv\")\n",
    "\n",
    "    dataset = ds.CSVDataset(csv_file, column_defaults=[str() for _ in range(785)], \n",
    "                            column_names=[\"label\"] + [\"pixel\" + str(i) for i in range(784)], shuffle=is_train)\n",
    "\n",
    "    # 数据预处理\n",
    "    def preprocess(*data):\n",
    "        label = np.array(data[0]).astype(np.int32)\n",
    "        image = np.array(data[1:]).reshape(28, 28).astype(np.float32) / 255.0\n",
    "        return image, label\n",
    "\n",
    "    dataset = dataset.map(operations=preprocess, input_columns=[\"label\"] + [\"pixel\" + str(i) for i in range(784)], \n",
    "                          output_columns=[\"image\", \"label\"])\n",
    "    dataset = dataset.project([\"image\", \"label\"])\n",
    "    dataset = dataset.map(operations=vision.Resize((32, 32)), input_columns=[\"image\"])\n",
    "    dataset = dataset.map(operations=vision.Rescale(1.0 / 255.0, 0.0), input_columns=[\"image\"])\n",
    "    dataset = dataset.map(operations=vision.Normalize(mean=[0.5], std=[0.5]), input_columns=[\"image\"])\n",
    "    dataset = dataset.map(operations=vision.HWC2CHW(), input_columns=[\"image\"])\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.repeat(repeat_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5b5d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回调函数定义\n",
    "class SaveBestModel(Callback):\n",
    "    def __init__(self, model, eval_dataset, eval_metric, model_dir):\n",
    "        self.model = model\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.eval_metric = eval_metric\n",
    "        self.model_dir = model_dir\n",
    "        self.best_acc = 0\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "        cb_params = run_context.original_args()\n",
    "        acc = self.model.eval(self.eval_dataset, dataset_sink_mode=False)[self.eval_metric]\n",
    "        if acc > self.best_acc:\n",
    "            self.best_acc = acc\n",
    "            if not os.path.exists(self.model_dir):\n",
    "                os.makedirs(self.model_dir)\n",
    "            mindspore.save_checkpoint(cb_params.train_network, os.path.join(self.model_dir, 'best.ckpt'))\n",
    "            print(f\"Model saved with {self.eval_metric}: {acc}\")\n",
    "\n",
    "class EpochLossMonitor(Callback):\n",
    "    def __init__(self, print_step=1):\n",
    "        super(EpochLossMonitor, self).__init__()\n",
    "        self.print_step = print_step\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "        cb_params = run_context.original_args()\n",
    "        epoch_num = cb_params.cur_epoch_num\n",
    "        loss = cb_params.net_outputs\n",
    "        if epoch_num % self.print_step == 0:\n",
    "            print(f\"Epoch {epoch_num}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f03fa",
   "metadata": {},
   "source": [
    "### 定义全连接神经网络 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8243a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(32 * 32, 128)\n",
    "        self.fc2 = nn.Dense(128, 64)\n",
    "        self.fc3 = nn.Dense(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db3791",
   "metadata": {},
   "source": [
    "#### 定义LeNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "898e0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Cell):\n",
    "    def __init__(self, num_class=10, num_channel=1):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
    "        self.fc1 = nn.Dense(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Dense(120, 84)\n",
    "        self.fc3 = nn.Dense(84, num_class)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.relu(self.max_pool2d(self.conv1(x)))\n",
    "        x = self.relu(self.max_pool2d(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab915be7",
   "metadata": {},
   "source": [
    "#### 定义AlexNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8722ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Cell):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.SequentialCell([\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        ])\n",
    "        self.classifier = nn.SequentialCell([\n",
    "            nn.Dropout(),\n",
    "            nn.Dense(256 * 4 * 4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Dense(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(1024, num_classes)\n",
    "        ])\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9361b",
   "metadata": {},
   "source": [
    "#### 定义VggNet网络 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe75eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.features = nn.SequentialCell([\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        ])\n",
    "        self.classifier = nn.SequentialCell([\n",
    "            nn.Dense(512 * 1 * 1, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Dense(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Dense(4096, 10)\n",
    "        ])\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa965ad1",
   "metadata": {},
   "source": [
    "#### 定义ResNet网络 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81c7b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Cell):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, has_bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, has_bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.SequentialCell()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.SequentialCell([\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, has_bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            ])\n",
    "\n",
    "    def construct(self, x):\n",
    "        out = ops.ReLU()(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = ops.ReLU()(out)\n",
    "        return out\n",
    "\n",
    "# 定义 ResNet18\n",
    "class ResNet(nn.Cell):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, has_bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Dense(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.SequentialCell(*layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        out = ops.ReLU()(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = ops.AdaptiveAvgPool2D((1, 1))(out)\n",
    "        out = ops.Flatten()(out)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bec5f88",
   "metadata": {},
   "source": [
    "#### 定义DenseNet网络 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78598edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Cell):\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4 * growth_rate, kernel_size=1, has_bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4 * growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, has_bias=False)\n",
    "\n",
    "    def construct(self, x):\n",
    "        out = self.conv1(ops.ReLU()(self.bn1(x)))\n",
    "        out = self.conv2(ops.ReLU()(self.bn2(out)))\n",
    "        out = ops.Concat(1)([out, x])\n",
    "        return out\n",
    "\n",
    "class Transition(nn.Cell):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, has_bias=False)\n",
    "\n",
    "    def construct(self, x):\n",
    "        out = self.conv(ops.ReLU()(self.bn(x)))\n",
    "        out = ops.AvgPool2D(2)(out)\n",
    "        return out\n",
    "\n",
    "class DenseNet(nn.Cell):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2 * growth_rate\n",
    "        self.conv1 = nn.Conv2d(1, num_planes, kernel_size=3, padding=1, has_bias=False)\n",
    "\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0] * growth_rate\n",
    "        self.trans1 = Transition(num_planes, int(num_planes * reduction))\n",
    "        num_planes = int(num_planes * reduction)\n",
    "\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1] * growth_rate\n",
    "        self.trans2 = Transition(num_planes, int(num_planes * reduction))\n",
    "        num_planes = int(num_planes * reduction)\n",
    "\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2] * growth_rate\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "        self.linear = nn.Dense(num_planes, num_classes)\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        for _ in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.SequentialCell(*layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.dense3(out)\n",
    "        out = ops.ReLU()(self.bn(out))\n",
    "        out = ops.AdaptiveAvgPool2D((1, 1))(out)\n",
    "        out = ops.Flatten()(out)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def DenseNet121():\n",
    "    return DenseNet(Bottleneck, [6, 12, 24], growth_rate=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0731f080",
   "metadata": {},
   "source": [
    "#### 训练和评估函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "990fd19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(net, train_dataset, test_dataset, model_dir, num_epochs, lr):\n",
    "    loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "    opt = nn.Momentum(net.trainable_params(), learning_rate=lr, momentum=0.9)\n",
    "    model = Model(net, loss_fn=loss, optimizer=opt, metrics={\"accuracy\"})\n",
    "\n",
    "    # 定义回调\n",
    "    time_cb = TimeMonitor(data_size=train_dataset.get_dataset_size())\n",
    "    loss_cb = EpochLossMonitor(print_step=1)\n",
    "    save_cb = SaveBestModel(model, test_dataset, 'accuracy', model_dir)\n",
    "\n",
    "    # 开始训练\n",
    "    model.train(num_epochs, train_dataset, callbacks=[time_cb, loss_cb, save_cb], dataset_sink_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c6677",
   "metadata": {},
   "source": [
    "#### 数据集下载与定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "411ecce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查数据集文件是否存在并包含数据...\n",
      "训练数据集文件大小: 133067916 bytes\n",
      "测试数据集文件大小: 37883023 bytes\n",
      "训练数据集大小: 1874 批次\n",
      "测试数据集大小: 312 批次\n"
     ]
    }
   ],
   "source": [
    "data_path = \"datasets/fashionMNIST\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# 下载并创建数据集CSV文件\n",
    "# create_fashion_mnist_csv(data_path)\n",
    "\n",
    "# 检查CSV文件是否正确创建并包含数据\n",
    "print(\"检查数据集文件是否存在并包含数据...\")\n",
    "train_csv_path = os.path.join(data_path, 'fashion-mnist_train.csv')\n",
    "test_csv_path = os.path.join(data_path, 'fashion-mnist_test.csv')\n",
    "print(f\"训练数据集文件大小: {os.path.getsize(train_csv_path)} bytes\")\n",
    "print(f\"测试数据集文件大小: {os.path.getsize(test_csv_path)} bytes\")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 加载数据集\n",
    "train_loader = create_csv_dataset(data_path, batch_size=batch_size, is_train=True)\n",
    "test_loader = create_csv_dataset(data_path, batch_size=batch_size, is_train=False)\n",
    "\n",
    "# 检查数据集是否加载成功\n",
    "print(f\"训练数据集大小: {train_loader.get_dataset_size()} 批次\")\n",
    "print(f\"测试数据集大小: {test_loader.get_dataset_size()} 批次\")\n",
    "\n",
    "num_epochs, lr = 100, 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b4576",
   "metadata": {},
   "source": [
    "#### 定义模型保存路径 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18137e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './results/mindspore_weight'\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d7557",
   "metadata": {},
   "source": [
    "#### 训练全连接神经网络 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63fba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全连接神经网络开始训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(103:140272838072128,MainProcess):2024-07-07-08:55:14.587.640 [mindspore/train/model.py:1106] For EpochLossMonitor callback, {'epoch_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] ME(103:140272838072128,MainProcess):2024-07-07-08:55:14.588.880 [mindspore/train/model.py:1106] For SaveBestModel callback, {'epoch_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n"
     ]
    }
   ],
   "source": [
    "print(\"全连接神经网络开始训练\")\n",
    "fc_net = FCNet()\n",
    "train_and_evaluate(fc_net, train_loader, test_loader, os.path.join(model_dir, 'FCNet.ckpt'),num_epochs,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d317866b",
   "metadata": {},
   "source": [
    "#### 训练LeNet卷积神经网络 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccc3d2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet神经网络开始训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11244:16732,MainProcess):2024-07-04-11:37:31.330.986 [mindspore\\train\\model.py:1118] For EpochLossMonitor callback, {'epoch_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n",
      "[WARNING] ME(11244:16732,MainProcess):2024-07-04-11:37:31.331.533 [mindspore\\train\\model.py:1118] For SaveBestModel callback, {'epoch_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeNet神经网络开始训练\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m Le_net \u001b[38;5;241m=\u001b[39m LeNet5()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLe_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLeNet.ckpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(net, train_dataset, test_dataset, model_dir, num_epochs, lr)\u001b[0m\n\u001b[0;32m      9\u001b[0m save_cb \u001b[38;5;241m=\u001b[39m SaveBestModel(model, test_dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, model_dir)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtime_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_sink_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\mindspore\\lib\\site-packages\\mindspore\\train\\model.py:1080\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, epoch, train_dataset, callbacks, dataset_sink_mode, sink_size, initial_epoch)\u001b[0m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks:\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_methods_for_custom_callbacks(callbacks, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1080\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset_sink_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_sink_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m            \u001b[49m\u001b[43msink_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msink_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m            \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;66;03m# When it's distributed training and using MindRT,\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;66;03m# the node id should be reset to start from 0.\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;66;03m# This is to avoid the timeout when finding the actor route tables in 'train' and 'eval' case(or 'fit').\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _enable_distributed_mindrt():\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\mindspore\\lib\\site-packages\\mindspore\\train\\model.py:115\u001b[0m, in \u001b[0;36m_save_final_ckpt.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\mindspore\\lib\\site-packages\\mindspore\\train\\model.py:628\u001b[0m, in \u001b[0;36mModel._train\u001b[1;34m(self, epoch, train_dataset, callbacks, dataset_sink_mode, sink_size, initial_epoch, valid_dataset, valid_frequency, valid_dataset_sink_mode)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_reuse_dataset(train_dataset)\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dataset_sink_mode:\n\u001b[1;32m--> 628\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcb_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_infos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mget_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_target\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    630\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe CPU cannot support dataset sink mode currently.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    631\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSo the training process will be performed with dataset not sink.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\mindspore\\lib\\site-packages\\mindspore\\train\\model.py:930\u001b[0m, in \u001b[0;36mModel._train_process\u001b[1;34m(self, epoch, train_dataset, list_callback, cb_params, initial_epoch, valid_infos)\u001b[0m\n\u001b[0;32m    928\u001b[0m list_callback\u001b[38;5;241m.\u001b[39mon_train_step_begin(run_context)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_network_mode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_network, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 930\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_network\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnext_element\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    931\u001b[0m cb_params\u001b[38;5;241m.\u001b[39mnet_outputs \u001b[38;5;241m=\u001b[39m outputs\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_scale_manager \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_scale_manager\u001b[38;5;241m.\u001b[39mget_drop_overflow_update():\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\mindspore\\lib\\site-packages\\mindspore\\nn\\cell.py:680\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_fn_registered():\n\u001b[0;32m    678\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCell\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms not support hook function in graph mode. If you want to use hook \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    679\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction, please use context.set_context to set pynative mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 680\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m    683\u001b[0m \u001b[38;5;66;03m# Run in PyNative mode.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\mindspore\\lib\\site-packages\\mindspore\\nn\\cell.py:1020\u001b[0m, in \u001b[0;36mCell.compile_and_run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile_and_run\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;124;03m    Compile and run Cell, the input must be consistent with the input defined in construct.\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m        Object, the result of executing.\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1020\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_flags(ge_sync_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1022\u001b[0m     new_args \u001b[38;5;241m=\u001b[39m _get_args_for_run(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\mindspore\\lib\\site-packages\\mindspore\\nn\\cell.py:997\u001b[0m, in \u001b[0;36mCell.compile\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynamic_shape_inputs \u001b[38;5;241m=\u001b[39m convert_inputs_to_dynamic(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynamic_shape_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 997\u001b[0m     \u001b[43m_cell_graph_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mjit_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_config_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_compile_dynamic_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynamic_shape_inputs, args)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\mindspore\\lib\\site-packages\\mindspore\\common\\api.py:1547\u001b[0m, in \u001b[0;36m_CellGraphExecutor.compile\u001b[1;34m(self, obj, phase, do_convert, jit_config_dict, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1545\u001b[0m     jit_config_dict \u001b[38;5;241m=\u001b[39m JitConfig()\u001b[38;5;241m.\u001b[39mjit_config_dict\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_executor\u001b[38;5;241m.\u001b[39mset_jit_config(jit_config_dict)\n\u001b[1;32m-> 1547\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_vm_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1548\u001b[0m obj\u001b[38;5;241m.\u001b[39mcompile_cache\u001b[38;5;241m.\u001b[39madd(phase)\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n",
      "\u001b[1;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"LeNet神经网络开始训练\")\n",
    "Le_net = LeNet5()\n",
    "train_and_evaluate(Le_net, train_loader, test_loader, os.path.join(model_dir, 'LeNet.ckpt'),num_epochs,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f01e43d",
   "metadata": {},
   "source": [
    "#### 训练AlexNet卷积神经网络 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f3bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AlexNet神经网络开始训练\")\n",
    "Alex_net = AlexNet()\n",
    "train_and_evaluate(Le_net, train_loader, test_loader, os.path.join(model_dir, 'AlexNet.ckpt'),num_epochs,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5c23a",
   "metadata": {},
   "source": [
    "#### 训练VggNet卷积神经网络 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"VggNet神经网络开始训练\")\n",
    "Vgg_net = VGGNet()\n",
    "train_and_evaluate(Le_net, train_loader, test_loader, os.path.join(model_dir, 'VggNet.ckpt'),num_epochs,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a47c84",
   "metadata": {},
   "source": [
    "#### 训练ResNet卷积神经网络 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c52705",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ResNet神经网络开始训练\")\n",
    "Vgg_net = ResNet18()\n",
    "train_and_evaluate(Le_net, train_loader, test_loader, os.path.join(model_dir, 'ResNet.ckpt'),num_epochs,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8672fdc",
   "metadata": {},
   "source": [
    "#### 训练DenseNet卷积神经网络 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee1f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DenseNet神经网络开始训练\")\n",
    "Dense_net = DenseNet121()\n",
    "train_and_evaluate(Le_net, train_loader, test_loader, os.path.join(model_dir, 'DenseNet.ckpt'),num_epochs,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a6a051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

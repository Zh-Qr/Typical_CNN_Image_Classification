{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf87fc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境准备完成\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tempfile\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"环境准备完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e750b2",
   "metadata": {},
   "source": [
    "## 数据集加载 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e7f0438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(mnist_train, mnist_test, batch_size):\n",
    "    if sys.platform.startswith('win'):\n",
    "        num_workers = 0\n",
    "    else:\n",
    "        num_workers = 4\n",
    "    train_iter = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_iter = DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5726ef9",
   "metadata": {},
   "source": [
    "## 加载数据集、参数设置 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72111cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /tmp/tmpzjmks6ag/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7799cd1ee5e64a3aa97f856975187208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tmpzjmks6ag/FashionMNIST/raw/train-images-idx3-ubyte.gz to /tmp/tmpzjmks6ag/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /tmp/tmpzjmks6ag/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e96be22ffa4be985b794a92ddab711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tmpzjmks6ag/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /tmp/tmpzjmks6ag/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /tmp/tmpzjmks6ag/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebd4cff93254e30aa589c81769b1466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tmpzjmks6ag/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/tmpzjmks6ag/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /tmp/tmpzjmks6ag/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7208be4c348c450a9bb1ff4ca54b6b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tmpzjmks6ag/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/tmpzjmks6ag/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用临时目录\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    train_loader = DataLoader(\n",
    "        datasets.FashionMNIST(\n",
    "            root=tmpdirname, \n",
    "            train=True, \n",
    "            transform=transforms.ToTensor(), \n",
    "            download=True\n",
    "        ), \n",
    "        batch_size=64, \n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        datasets.FashionMNIST(\n",
    "            root=tmpdirname, \n",
    "            train=False, \n",
    "            transform=transforms.ToTensor(), \n",
    "            download=True\n",
    "        ), \n",
    "        batch_size=64, \n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "        root=tmpdirname, \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transforms.ToTensor()\n",
    "    )\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "        root=tmpdirname, \n",
    "        train=False, \n",
    "        download=True, \n",
    "        transform=transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter = load_data_fashion_mnist(mnist_train, mnist_test, batch_size)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "save_dir = 'results/process_picture/'\n",
    "weight_dir = 'results/weight/'\n",
    "\n",
    "#训练参数\n",
    "lr, num_epochs = 0.001, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c394c368",
   "metadata": {},
   "source": [
    "## 训练函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcbbd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    train_loss_list, train_acc_list, test_acc_list = [], [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        train_loss_list.append(train_l_sum / batch_count)\n",
    "        train_acc_list.append(train_acc_sum / n)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
    "    return train_loss_list, train_acc_list, test_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b99de1",
   "metadata": {},
   "source": [
    "## 效果分析函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61259395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            net.eval() # 评估模式, 这会关闭dropout\n",
    "            acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "            net.train() # 改回训练模式\n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc2c8a6",
   "metadata": {},
   "source": [
    "## 训练过程可视化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0a7f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_process(num_epochs, train_loss_list, train_acc_list, test_acc_list,save_name):\n",
    "    # 绘制训练过程的损失和准确率\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss_list, label='Training loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_acc_list, label='Training accuracy')\n",
    "    plt.plot(epochs, test_acc_list, label='Test accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # 创建保存目录\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 生成文件路径\n",
    "    file_path = os.path.join(save_dir, save_name)\n",
    "\n",
    "    # 保存图片\n",
    "    plt.savefig(file_path)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Plot saved to {file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7118681",
   "metadata": {},
   "source": [
    "### 搭建全连接神经网络 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9eaeb",
   "metadata": {},
   "source": [
    "#### 网络搭建 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "844daec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)  # Flatten the input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90401686",
   "metadata": {},
   "source": [
    "#### 编译神经网络并开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65bd653d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 1.4069\n",
      "Epoch 2/3, Loss: 0.7295\n",
      "Epoch 3/3, Loss: 0.6054\n"
     ]
    }
   ],
   "source": [
    "# 实例化网络\n",
    "network = Network()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# 设置优化器\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.01)\n",
    "\n",
    "# 训练网络\n",
    "loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = network(images)\n",
    "        loss = loss_func(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    loss_list.append(avg_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a78c0",
   "metadata": {},
   "source": [
    "#### 可视化训练过程 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba588490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to results/process_picture/FC.png\n"
     ]
    }
   ],
   "source": [
    "# 可视化训练损失\n",
    "plt.plot(loss_list, label='Training Loss')\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# 创建保存目录\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# 生成文件路径\n",
    "file_path = os.path.join(save_dir, 'FC.png')\n",
    "\n",
    "# 保存图片\n",
    "plt.savefig(file_path)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Plot saved to {file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f17c364",
   "metadata": {},
   "source": [
    "#### 保存模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06f0cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = os.path.join(weight_dir,'FC_mnist.pth')\n",
    "torch.save(network,weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff09e28",
   "metadata": {},
   "source": [
    "### 搭建LeNET卷积神经网络 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a56d1",
   "metadata": {},
   "source": [
    "#### 网络搭建 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b32fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16*4*4, 120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(img.shape[0], -1))\n",
    "        return output\n",
    "\n",
    "#网络实例化\n",
    "lenet = LeNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb11b15",
   "metadata": {},
   "source": [
    "#### 开始训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84ed3521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.6897, train acc 0.737, test acc 0.744, time 9.2 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(lenet\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m----> 2\u001b[0m train_loss_list, train_acc_list, test_acc_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlenet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      7\u001b[0m     train_l_sum, train_acc_sum, n, batch_count, start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_iter:\n\u001b[0;32m      9\u001b[0m         X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m         y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    167\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbands\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    170\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(lenet.parameters(), lr=lr)\n",
    "train_loss_list, train_acc_list, test_acc_list = train(lenet, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23c050",
   "metadata": {},
   "source": [
    "#### 可视化训练过程 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "875652f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_process(num_epochs,\u001b[43mtrain_loss_list\u001b[49m,train_acc_list)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loss_list' is not defined"
     ]
    }
   ],
   "source": [
    "train_process(num_epochs,train_loss_list,train_acc_list,test_acc_list,'LeNet.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d789f4",
   "metadata": {},
   "source": [
    "#### 保存模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe5744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = os.path.join(weight_dir,'LeNET_mnist.pth')\n",
    "torch.save(lenet,weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a14fb4",
   "metadata": {},
   "source": [
    "### 搭建AlexNET卷积神经网络 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4133efa",
   "metadata": {},
   "source": [
    "#### 网络搭建 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b335974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),  # 1 通道改为 64\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 3 * 3, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "    \n",
    "#网络实例化\n",
    "alexnet = AlexNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c5f10b",
   "metadata": {},
   "source": [
    "#### 开始训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07f4b510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.5492, train acc 0.792, test acc 0.869, time 15.1 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(alexnet\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m----> 2\u001b[0m train_loss_list, train_acc_list, test_acc_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43malexnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      7\u001b[0m     train_l_sum, train_acc_sum, n, batch_count, start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_iter:\n\u001b[0;32m      9\u001b[0m         X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m         y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    167\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbands\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    170\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(alexnet.parameters(), lr=lr)\n",
    "train_loss_list, train_acc_list, test_acc_list = train(alexnet, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff047dd5",
   "metadata": {},
   "source": [
    "#### 可视化训练过程 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_process(num_epochs,train_loss_list,train_acc_list,test_acc_list,'AlexNet.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b91f40",
   "metadata": {},
   "source": [
    "#### 保存训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aad9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = os.path.join(weight_dir,'Alexnet_mnist.pth')\n",
    "torch.save(alexnet,weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655b388",
   "metadata": {},
   "source": [
    "### 搭建VggNet卷积神经网络 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50539823",
   "metadata": {},
   "source": [
    "#### 网络搭建 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b24ec4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义简化的 VGG 网络\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 1 * 1, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "#网络实例化\n",
    "vggnet = VGGNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1137a0",
   "metadata": {},
   "source": [
    "####  开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e5f46c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.6182, train acc 0.762, test acc 0.848, time 27.7 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(vggnet\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m----> 2\u001b[0m train_loss_list, train_acc_list, test_acc_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvggnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m     14\u001b[0m l\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 16\u001b[0m train_l_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     17\u001b[0m train_acc_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (y_hat\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     18\u001b[0m n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(vggnet.parameters(), lr=lr)\n",
    "train_loss_list, train_acc_list, test_acc_list = train(vggnet, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cee649c",
   "metadata": {},
   "source": [
    "#### 训练过程可视化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7207b40a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_process(num_epochs,\u001b[43mtrain_loss_list\u001b[49m,train_acc_list)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loss_list' is not defined"
     ]
    }
   ],
   "source": [
    "train_process(num_epochs,train_loss_list,train_acc_list,test_acc_list,'VggNet.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a620232",
   "metadata": {},
   "source": [
    "#### 保存模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72c7b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = os.path.join(weight_dir,'vggnet_mnist.pth')\n",
    "torch.save(vggnet,weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea0b36",
   "metadata": {},
   "source": [
    "### 搭建resnet卷积神经网络 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8aa363",
   "metadata": {},
   "source": [
    "#### 网络搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20a14e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义基本的残差块\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512 * block.expansion * 4 * 4, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "resnet = ResNet18().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739431e2",
   "metadata": {},
   "source": [
    "#### 开始训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82bbb32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.5200, train acc 0.831, test acc 0.882, time 55.8 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(resnet\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m----> 2\u001b[0m train_loss_list, train_acc_list, test_acc_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      7\u001b[0m     train_l_sum, train_acc_sum, n, batch_count, start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_iter:\n\u001b[0;32m      9\u001b[0m         X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m         y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    167\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbands\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    170\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=lr)\n",
    "train_loss_list, train_acc_list, test_acc_list = train(resnet, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b2c23",
   "metadata": {},
   "source": [
    "#### 训练过程可视化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cbfcfa8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_process(num_epochs,\u001b[43mtrain_loss_list\u001b[49m,train_acc_list)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loss_list' is not defined"
     ]
    }
   ],
   "source": [
    "train_process(num_epochs,train_loss_list,train_acc_list,test_acc_list,'ResNet.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2b965",
   "metadata": {},
   "source": [
    "#### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b261f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = os.path.join(weight_dir,'resnet_mnist.pth')\n",
    "torch.save(resnet,weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb205e11",
   "metadata": {},
   "source": [
    "### 搭建DenseNet卷积神经网络 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ddb82a",
   "metadata": {},
   "source": [
    "#### 网络搭建 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91ff1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, growth_rate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, 4 * growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(4 * growth_rate)\n",
    "        self.conv2 = nn.Conv2d(4 * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(torch.relu(self.bn1(x)))\n",
    "        out = self.conv2(torch.relu(self.bn2(out)))\n",
    "        out = torch.cat([out, x], 1)\n",
    "        return out\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_planes)\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(torch.relu(self.bn(x)))\n",
    "        out = nn.functional.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        num_planes = 2 * growth_rate\n",
    "        self.conv1 = nn.Conv2d(1, num_planes, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n",
    "        num_planes += nblocks[0] * growth_rate\n",
    "        self.trans1 = Transition(num_planes, num_planes // 2)\n",
    "        num_planes = num_planes // 2\n",
    "\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1] * growth_rate\n",
    "        self.trans2 = Transition(num_planes, num_planes // 2)\n",
    "        num_planes = num_planes // 2\n",
    "\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2] * growth_rate\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(num_planes)\n",
    "        self.linear = nn.Linear(1536, num_classes)\n",
    "  # 修改全连接层输入维度\n",
    "\n",
    "    def _make_dense_layers(self, block, in_planes, nblock):\n",
    "        layers = []\n",
    "        for _ in range(nblock):\n",
    "            layers.append(block(in_planes, self.growth_rate))\n",
    "            in_planes += self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"Input shape:\", x.shape)\n",
    "        out = self.conv1(x)\n",
    "        # print(\"After Conv1:\", out.shape)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        # print(\"After Dense1 and Trans1:\", out.shape)\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        # print(\"After Dense2 and Trans2:\", out.shape)\n",
    "        out = self.dense3(out)\n",
    "        # print(\"After Dense3:\", out.shape)\n",
    "        out = torch.relu(self.bn(out))\n",
    "        out = nn.functional.avg_pool2d(out, 3)\n",
    "        # print(\"After final avg pool:\", out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # print(\"Before Linear:\", out.shape)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def DenseNet121():\n",
    "    return DenseNet(Bottleneck, [6, 12, 24], growth_rate=12)\n",
    "\n",
    "densenet = DenseNet121().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde1f20",
   "metadata": {},
   "source": [
    "#### 开始训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95311fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.3751, train acc 0.861, test acc 0.889, time 70.3 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(densenet\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m----> 2\u001b[0m train_loss_list, train_acc_list, test_acc_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdensenet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      7\u001b[0m     train_l_sum, train_acc_sum, n, batch_count, start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_iter:\n\u001b[0;32m      9\u001b[0m         X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m         y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    167\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbands\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    170\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(densenet.parameters(), lr=lr)\n",
    "train_loss_list, train_acc_list, test_acc_list = train(densenet, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817131f3",
   "metadata": {},
   "source": [
    "#### 训练过程可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_process(num_epochs,train_loss_list,train_acc_list, test_acc_list,'DenseNet.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da7aa0",
   "metadata": {},
   "source": [
    "#### 保存模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4fec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = os.path.join(weight_dir,'densenet_mnist.pth')\n",
    "torch.save(densenet,weight_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
